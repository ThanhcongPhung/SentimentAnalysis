# -*- coding: utf-8 -*-
"""sentiment2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14RCRuFGU4mmAaDcsLFPDVwHhTeLS2NQb
"""

pip install visen

pip install fasttext

pip install pyvi

import pandas as pd
import re
import numpy as np
from gensim.models import Word2Vec
import tensorflow as tf
from gensim.models import FastText
from gensim.models import KeyedVectors
from keras.models import Sequential
from keras.layers import Dense, Dropout, Bidirectional, Conv1D,MaxPooling1D,Flatten, SpatialDropout1D,Concatenate
from keras.layers.embeddings import Embedding
from keras.layers.recurrent import GRU,LSTM
from gensim.models import Word2Vec
from sklearn.metrics import precision_recall_fscore_support as score
import fasttext
import time
import visen
from pyvi import ViTokenizer, ViPosTagger

colnames = ['id', 'free_text']
data1 = pd.read_csv('/content/drive/My Drive/SentimentAnalysis/data/02_train_text.csv', names=colnames)

colnames2 = ['id', 'label_id']
data2 = pd.read_csv('/content/drive/My Drive/SentimentAnalysis/data/03_train_label.csv')
df = pd.merge(data1, data2, on="id", how="inner")
del df['id']
data3 = pd.read_csv('/content/drive/My Drive/SentimentAnalysis/data/06_test_text_label.csv')
label_test = data3.label.to_list()
text_test = data3.free_text.to_list()

# Divide dataset to 8/2 train, test for label 0,1,2
Df1 = pd.DataFrame()
Df1['text_0'] = df['free_text'].groupby(df['label_id']).apply(list)[0]
np.shape(Df1['text_0'])
Df1['label_0'] = np.zeros((18614,), dtype=np.int32)

Df2 = pd.DataFrame()
Df2['text_1'] = df['free_text'].groupby(df['label_id']).apply(list)[1]
np.shape(Df2['text_1'])
Df2['label_1'] = np.ones((1022,), dtype=np.int32)

Df3 = pd.DataFrame()
Df3['text_2'] = df['free_text'].groupby(df['label_id']).apply(list)[2]
Df3['label_2'] = np.full(shape=709, fill_value=2, dtype=np.int32)

from sklearn.model_selection import train_test_split

x_data_0 = Df1['text_0']
y_data_0 = Df1['label_0']
x_train_0, x_test_0, y_train_0, y_test_0 = train_test_split(np.array(x_data_0), np.array(y_data_0), test_size=0.2,
                                                            shuffle=False)

x_data_1 = Df2['text_1']
y_data_1 = Df2['label_1']
x_train_1, x_test_1, y_train_1, y_test_1 = train_test_split(np.array(x_data_1), np.array(y_data_1), test_size=0.2,
                                                            shuffle=False)

x_data_2 = Df3['text_2']
y_data_2 = Df3['label_2']
x_train_2, x_test_2, y_train_2, y_test_2 = train_test_split(np.array(x_data_2), np.array(y_data_2), test_size=0.2,
                                                            shuffle=False)

X_train = np.concatenate((x_train_0, x_train_1, x_train_2), axis=None)
X_test = np.concatenate((x_test_0, x_test_1, x_test_2), axis=None)
Y_test = np.concatenate((y_test_0, y_test_1, y_test_2), axis=None)
Y_train = np.concatenate((y_train_0, y_train_1, y_train_2), axis=None)

x_data_1 = [each_string.lower() for each_string in x_data_1]
X_data_1_1= normalize_text(x_data_1)
X_data_1_2 = cleanup_text(X_data_1_1)
X_data_1_2[101:500]

Y_train_label = np.concatenate((Y_train, Y_test), axis=None)

X_train = [each_string.lower() for each_string in X_train]
X_test = [each_string.lower() for each_string in X_test]
text_test = [each_string.lower() for each_string in text_test]

abbrevs = {r'\bk\b': 'không', r'\bko\b': 'không',r"\bkh\b": 'không', r"\b's\b": ' ', r"\be\b": 'em', r"\bsđt\b": 'điện thoại',
           r"\bib\b": 'inbox', r"\bctv\b": 'cộng tác viên',r"\bđkm\b": 'dm',r"\bvcl\b": 'vl',r"\bvclin\b": 'vl',r"\bđm\b": 'dm',r"\bvkl\b": 'vl',r"\bd.m\b": 'dm',r"\bdmm\b": 'dm',r"\bđcm\b": 'dm',,r"\bđmm\b": 'dm',
           r"\bhnoi\b": 'hà nội',r"\bhanoi\b": 'hà nội',r"\bfb\b": 'facebook',r"\bcmt\b": 'comment',r"\bcoment\b": 'comment',r"\bstt\b": 'status'}


def replace_all(text, dic):
    for i, j in dic.items():
        text = re.sub(i, j, text)
    return text


def normalize_text(sentences):
    normalize = []
    for sent in sentences:
        normalize.append(replace_all(sent, abbrevs))
    return normalize


first_clean = normalize_text(X_train)
first_clean_test = normalize_text(X_test)
first_clean_text_test = normalize_text(text_test)


def cleanup_text(sentences):
    cleaned_text = []
    for sent in sentences:
        # get hastag
        sent = re.sub(r'(?:^|\s)(\#\w+)', ' hastag', sent)
        # get email
        sent = re.sub(r'[\w\.-]+@[\w\.-]+', 'email', sent)
        # get url
        sent = re.sub(r'((http|https)\:\/\/)?[a-zA-Z0-9\.\/\?\:@\-_=#]+\.([a-zA-Z]){2,6}([a-zA-Z0-9\.\&\/\?\:@\-_=#])*',
                      ' url', sent)
        # dupplicate word
        # sent = re.sub(r'(\w)\1+', r'\1', sent)
        # delete string that contain number
        sent = re.sub(r"\w*\d\w*", ' ', sent)

        # y = re.sub(r'\b\d+([\.,]\d+)?', ' number', g)
        # delete number in string
        # g = re.sub(r"\d*([^\d\W]+)\d*", r'\1', z)
        sent = re.sub('[^\w ]', ' ', sent)
        sent = visen.clean_tone(sent)
        cleaned_text.append(sent)
    return cleaned_text


second_clean = cleanup_text(first_clean)
second_clean_test = cleanup_text(first_clean_test)
second_clean_text_test = cleanup_text(first_clean_text_test)


# tokenizer for sentencse
def tokenize_sentences(sentences):
    tokens_list = []
    for sent in sentences:
        tokens = ViTokenizer.tokenize(sent)
        tokens_list.append(tokens)
    return tokens_list


third_clean = tokenize_sentences(second_clean)
third_clean_test = tokenize_sentences(second_clean_test)
third_clean_text_test = tokenize_sentences(second_clean_text_test)

with open("/content/drive/My Drive/SentimentAnalysis/vietnamese-stopwords.txt", encoding="utf8") as f:
    stop_word = f.readlines()
stop_word = [x.strip() for x in stop_word]


def remove_stop_words(corpus):
    removed_stop_words = []
    for review in corpus:
        removed_stop_words.append(
            ' '.join([word for word in review.split()
                      if word not in stop_word])
        )

    return removed_stop_words
remove_stop = remove_stop_words(third_clean)
remove_stop_test = remove_stop_words(third_clean_test)

def get_train_input(train_texts):
    train_input = []
    model = fasttext.load_model("/content/drive/My Drive/SentimentAnalysis/models/cc.vi.300.bin")
    for line in train_texts:
        embedding = []
        for x in range(100):
            if len(line) <= x:
                embedding.append(np.zeros(300, dtype=np.float32))
            else:
                # try:
                c = model.get_word_vector(line[x])
                # except KeyError:
                #     c = np.zeros(80, dtype=np.float32)
                embedding.append(c)
        train_input.append(embedding)
    return train_input

def split_word(sentences):
    new_arr = []
    for sent in sentences:
        new_arr.append(sent.split())
    return new_arr


fourth_clean = split_word(remove_stop)
fourth_clean_test = split_word(remove_stop_test)
# fourth_clean_train = np.concatenate((fourth_clean, fourth_clean_test), axis=None)
# fourth_clean_text_test = tokenize_sentences(third_clean_text_test)

start_time = time.time()
# X_text_train = np.array(get_train_input(fourth_clean_train))
# X_text_test = np.array(get_train_input(fourth_clean_text_test))
# X_label = tf.keras.utils.to_categorical(Y_train_label, num_classes=3)
X_text_train = np.array(get_train_input(fourth_clean))
X_text_test= np.array(get_train_input(fourth_clean_test))
X_label_train = tf.keras.utils.to_categorical(Y_train, num_classes=3)

input_dim = (X_text_train.shape[1], X_text_train.shape[2])

from keras import backend as K
def recall_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))
        recall = true_positives / (possible_positives + K.epsilon())
        return recall


def precision_m(y_true, y_pred):
        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))
        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))
        precision = true_positives / (predicted_positives + K.epsilon())
        return precision


def f1_m(y_true, y_pred):
        precision = precision_m(y_true, y_pred)
        recall = recall_m(y_true, y_pred)
        return 2 * ((precision * recall) / (precision + recall + K.epsilon()))

# build model
model = Sequential()
model.add(Bidirectional(GRU(128, return_sequences=True), input_shape=input_dim))
model.add(Dropout(0.2))
# model.add(Bidirectional(GRU(32)))
model.add(Conv1D(filters=100, kernel_size=3, activation='relu', input_shape=input_dim))
# model.add(Dropout(0.2))
# model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))
model.add(Dropout(0.2))
model.add(MaxPooling1D(pool_size=2))
model.add(Flatten())

model.add(Dense(100, activation='relu'))
model.add(Dense(3, activation='softmax'))
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', f1_m, precision_m, recall_m])
model.summary()
history = model.fit(X_text_train, X_label_train, epochs=10, batch_size=64,verbose=1)

print()
print("Execution Time %s seconds: " % (time.time() - start_time))
from sklearn.metrics import classification_report

start_time_test = time.time()
prediction = model.predict(X_text_test)
label = []
for n in prediction:
    label.append(np.argmax(n))
# print(prediction)

print(classification_report(Y_test, label))
print()
print("Execution Time test %s seconds: " % (time.time() - start_time_test))

# text_1=['Quản Tùng Lâm đcm công việc tốt vl =)))','em nói cho chị cổ lực na trát nghe ở bển chị cứ lo mần ăn đừng có hơn thua với mấy con đĩ này làm gì mấy con đĩ này là mấy con đĩ nghèo khổ khi nào chị dề việt nam đi em cho chị một miếng đất em có 7 miếng lận nè 1 miếng nè 2 miếng nè 3 miếng nè 4 miếng nè 5 miếng nè 6 miếng nè 7 miếng nè','Mày làm 1 tháng 10 triệu ở Việt Nam là ổn rồi, còn kêu ca gì nữa. Mày muốn gây bạo động lật đổ chính quyền à ? Thế thì lực lượng Công an ra tay dẹp loạn ngay. Mùa xuân dân chủ của lũ mày trong tù giam chứ không được thảnh thơi đâu. Mày so bì con nhà giàu, bộ mày không muốn dân Việt Nam giàu à hay muốn toàn dân nhà nghèo ?','thằng này óc chó vl','Kệ tao -_- sân si vl','Đề nghị đã đăng bọn rác rưởi phân hôi , cần thiết phải hình ảnh không bị che , như sex JAV','Sống trên đời này thì chỉ có tình thương mến thương . Chứ nói về độ sợ nhau thì thằng cha mày bố mày còn tát tai chứ mày ??  <URL>']
text_test = [each_string.lower() for each_string in text_test]
text_2= normalize_text(text_test)
text_3 = cleanup_text(text_2)
text_4= tokenize_sentences(text_3)
stop_w = remove_stop_words(text_4)
text_5 = split_word(stop_w)
text_6 = np.array(get_train_input(text_5))

x=model.predict(text_6)
y=[]
for n in x:
    y.append(np.argmax(n))
y

print(classification_report(label_test, y))

remove_stop[:14891]